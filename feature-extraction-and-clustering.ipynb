{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7217be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\SUCI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\SUCI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import warnings\n",
    "import string\n",
    "import nltk\n",
    "from time import time\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from sklearn import metrics\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec, FastText\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from collections import defaultdict\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1948095e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773b6637",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction and Clustering using experiment I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ac86e0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['namaproduk', 'desc', 'harga', 'terjual', 'rating', 'namatoko',\n",
      "       'kategori', 'subkategori', 'marketplace'],\n",
      "      dtype='object')\n",
      "1959 documents - 5 categories\n",
      "['ATK' 'Elektronik' 'Kesehatan' 'Kosmetik' 'Pakaian']\n",
      "vectorization done in 0.118 s\n",
      "n_samples: 1959, n_features: 1922\n",
      "\n",
      "*** KMeans with LSA on tf-idf vectors:\n",
      "True number of documents in each category according to the class labels: [391 392 380 397 399]\n",
      "Performing dimensionality reduction using LSA ...\n",
      "LSA done in 0.169 s\n",
      "Explained variance of the SVD step: 59.3%\n",
      "Number of elements asigned to each cluster: [717 270 417 274 281], sse= 151582, seed= 0\n",
      "Number of elements asigned to each cluster: [ 95 271 200 940 453], sse= 432422, seed= 1\n",
      "Number of elements asigned to each cluster: [274 694 191 154 646], sse= 260672, seed= 2\n",
      "Number of elements asigned to each cluster: [196 491 849 271 152], sse= 344672, seed= 3\n",
      "Number of elements asigned to each cluster: [466 199 287 522 485], sse= 74544, seed= 4\n",
      "clustering done in 0.10 ± 0.01 s \n",
      "Homogeneity: 0.576 ± 0.073\n",
      "Completeness: 0.636 ± 0.052\n",
      "V-measure: 0.604 ± 0.063\n",
      "Adjusted Rand-Index: 0.455 ± 0.087\n",
      "Silhouette Coefficient: 0.098 ± 0.002\n",
      "Number of elements asigned to each cluster: [466 199 287 522 485], sse= 74544\n",
      "(1959, 5)\n",
      "(1959,)\n",
      "(1959,)\n"
     ]
    }
   ],
   "source": [
    "nmfile = 'data-after-preprocess-experiment1'\n",
    "df = pd.read_csv(nmfile+'.csv')\n",
    "nmkolom = df.columns\n",
    "\n",
    "print (nmkolom)\n",
    "\n",
    "kolomproduk = 'namaproduk'\n",
    "kolomdeskripsi = 'desc'\n",
    "\n",
    "nmproduk = df[kolomproduk].tolist()\n",
    "deskripsi = df[kolomdeskripsi].tolist()\n",
    "\n",
    "kategori = df['kategori'].tolist()\n",
    "subkategori = df['subkategori'].tolist()\n",
    "\n",
    "rawdataset = []\n",
    "\n",
    "def get_dataset():\n",
    "    global df\n",
    "    df = df.reset_index()  # make sure indexes pair with number of rows\n",
    "    testdata = ''\n",
    "    i = 0\n",
    "    iambil = 1650\n",
    "    sentences = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        sebaris1 = row[kolomproduk]\n",
    "        if (pd.isna(sebaris1)):\n",
    "            sebaris1 = ' '\n",
    "\n",
    "        sebaris2 = row[kolomdeskripsi]\n",
    "        if (pd.isna(sebaris2)):\n",
    "            sebaris2 = ' '\n",
    "\n",
    "        kalimat = sebaris1 + ' ' + sebaris2\n",
    "        if (i==iambil):\n",
    "            testdata = kalimat\n",
    "            \n",
    "        sentences.append(kalimat)\n",
    "        i = i + 1\n",
    "\n",
    "    return (testdata, sentences)\n",
    "\n",
    "testdata, dataset_data = get_dataset()\n",
    "\n",
    "\n",
    "labels = kategori\n",
    "unique_labels, category_sizes = np.unique(labels, return_counts=True)\n",
    "true_k = unique_labels.shape[0]\n",
    "\n",
    "print(f\"{len(dataset_data)} documents - {true_k} categories\")\n",
    "print (unique_labels)\n",
    "\n",
    "\n",
    "evaluations = []\n",
    "evaluations_std = []\n",
    "\n",
    "\n",
    "def fit_and_evaluate(km, X, kateg_sizes, name=None, n_runs=5):\n",
    "    name = km._class.name_ if name is None else name\n",
    "\n",
    "    train_times = []\n",
    "    scores = defaultdict(list)\n",
    "    seed_sse_terkecil = 0\n",
    "    sse_lama = 999999\n",
    "    for seed in range(n_runs):\n",
    "        km.set_params(random_state=seed)\n",
    "        t0 = time()\n",
    "        km.fit(X)\n",
    "        train_times.append(time() - t0)\n",
    "        cluster_ids, cluster_sizes = np.unique(km.labels_, return_counts=True)\n",
    "        sse = 0\n",
    "        for i in range(len(kateg_sizes)):\n",
    "            sse = sse + pow(cluster_sizes[i] - kateg_sizes[i], 2)\n",
    "            \n",
    "        if (sse<sse_lama):\n",
    "            seed_sse_terkecil = seed \n",
    "        sse_lama = sse \n",
    "        \n",
    "        print(f\"Number of elements asigned to each cluster: {cluster_sizes}, sse= {sse}, seed= {seed}\") \n",
    "\n",
    "        scores[\"Homogeneity\"].append(metrics.homogeneity_score(labels, km.labels_))\n",
    "        scores[\"Completeness\"].append(metrics.completeness_score(labels, km.labels_))\n",
    "        scores[\"V-measure\"].append(metrics.v_measure_score(labels, km.labels_))\n",
    "        scores[\"Adjusted Rand-Index\"].append(\n",
    "            metrics.adjusted_rand_score(labels, km.labels_)\n",
    "        )\n",
    "        scores[\"Silhouette Coefficient\"].append(\n",
    "            metrics.silhouette_score(X, km.labels_, sample_size=2000)\n",
    "        )\n",
    "    train_times = np.asarray(train_times)\n",
    "\n",
    "    print(f\"clustering done in {train_times.mean():.2f} ± {train_times.std():.2f} s \")\n",
    "    evaluation = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.mean(),\n",
    "    }\n",
    "    evaluation_std = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.std(),\n",
    "    }\n",
    "    for score_name, score_values in scores.items():\n",
    "        mean_score, std_score = np.mean(score_values), np.std(score_values)\n",
    "        print(f\"{score_name}: {mean_score:.3f} ± {std_score:.3f}\")\n",
    "        evaluation[score_name] = mean_score\n",
    "        evaluation_std[score_name] = std_score\n",
    "    evaluations.append(evaluation)\n",
    "    evaluations_std.append(evaluation_std)\n",
    "    return seed_sse_terkecil\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=5, stop_words=\"english\")\n",
    "t0 = time()\n",
    "X_tfidf = vectorizer.fit_transform(dataset_data)\n",
    "\n",
    "print(f\"vectorization done in {time() - t0:.3f} s\")\n",
    "print(f\"n_samples: {X_tfidf.shape[0]}, n_features: {X_tfidf.shape[1]}\")\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '8'\n",
    "\n",
    "print (\"\\n*** KMeans with LSA on tf-idf vectors:\")\n",
    "print(\n",
    "    \"True number of documents in each category according to the class labels: \"\n",
    "    f\"{category_sizes}\"\n",
    ")\n",
    "\n",
    "print (\"Performing dimensionality reduction using LSA ...\")\n",
    "lsa = make_pipeline(TruncatedSVD(n_components=100), Normalizer(copy=False))\n",
    "t0 = time()\n",
    "X_lsa = lsa.fit_transform(X_tfidf)\n",
    "explained_variance = lsa[0].explained_variance_ratio_.sum()\n",
    "\n",
    "print(f\"LSA done in {time() - t0:.3f} s\")\n",
    "print(f\"Explained variance of the SVD step: {explained_variance * 100:.1f}%\")\n",
    "\n",
    "kmeans = KMeans(n_clusters=true_k, max_iter=100, n_init=5)\n",
    "\n",
    "seed_pilih = fit_and_evaluate(kmeans, X_lsa, category_sizes, name=\"KMeans\\non tf-idf vectors\")\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=true_k, max_iter=100, n_init=5, random_state=seed_pilih)\n",
    "kmeans.fit(X_lsa)\n",
    "centroids = kmeans.cluster_centers_\n",
    "cluster_ids, cluster_sizes = np.unique(kmeans.labels_, return_counts=True)\n",
    "labels = kmeans.labels_\n",
    "sse = 0\n",
    "for i in range(len(category_sizes)):\n",
    "    sse = sse + pow(cluster_sizes[i] - category_sizes[i], 2)\n",
    "\n",
    "print(f\"Number of elements asigned to each cluster: {cluster_sizes}, sse= {sse}\")\n",
    "\n",
    "def distance_from_center(X, centroids):\n",
    "\n",
    "\tdistance=euclidean_distances(X, np.array(centroids) )\n",
    "\n",
    "\thasil = np.min(distance, axis=1)\n",
    "\tprint (distance.shape)\n",
    "\tprint (hasil.shape)\n",
    "\n",
    "\treturn hasil   \n",
    "    \n",
    "euc_res=distance_from_center(X_lsa, centroids)\n",
    "\n",
    "# normalize the result  \n",
    "normlaized_res= euc_res  \n",
    "dfx = pd.DataFrame(normlaized_res, columns=columnName)\n",
    "dfx['cluster'] = labels\n",
    "dfx['kategori'] = kategori\n",
    "\n",
    "print (normlaized_res.shape)\n",
    "\n",
    "jumlahNpertama = 50\n",
    "\n",
    "for clusterx in cluster_ids:\n",
    "    dfy = dfx[dfx['cluster'] == clusterx]\n",
    "    dfz = dfy.sort_values(by=[nmkol], ascending=False)\n",
    "    # misal ditampilkan top ten outliers\n",
    "    outliers_idx = dfz.head(jumlahNpertama).index\n",
    "    #outliers = dfx[dfx.index.isin(outliers_idx)]\n",
    "    for idx in outliers_idx:\n",
    "        distance  = dfx.iloc[idx][nmkol]  \n",
    "        kategorix = df.iloc[idx]['kategori']   \n",
    "        nmproduk = df.iloc[idx]['namaproduk']\n",
    "        nmproduk = nmproduk[0:30]\n",
    "        \n",
    "dfx.to_csv(nmfile+'-cluster-LSA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a09e05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169d3691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction and Clustering using experiment II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "020edeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['namaproduk', 'desc', 'harga', 'terjual', 'rating', 'namatoko',\n",
      "       'kategori', 'subkategori', 'marketplace'],\n",
      "      dtype='object')\n",
      "1984 documents - 5 categories\n",
      "['ATK' 'Elektronik' 'Kesehatan' 'Kosmetik' 'Pakaian']\n",
      "vectorization done in 0.179 s\n",
      "n_samples: 1984, n_features: 2942\n",
      "\n",
      "*** KMeans with LSA on tf-idf vectors:\n",
      "True number of documents in each category according to the class labels: [394 398 395 398 399]\n",
      "Performing dimensionality reduction using LSA ...\n",
      "LSA done in 0.249 s\n",
      "Explained variance of the SVD step: 50.2%\n",
      "Number of elements asigned to each cluster: [390  98 293 808 395], sse= 268536, seed= 0\n",
      "Number of elements asigned to each cluster: [286 640 284 390 384], sse= 82838, seed= 1\n",
      "Number of elements asigned to each cluster: [289 620 583  98 394], sse= 185678, seed= 2\n",
      "Number of elements asigned to each cluster: [275 382 941 287  99], sse= 414854, seed= 3\n",
      "Number of elements asigned to each cluster: [199 701 701 196 187], sse= 309218, seed= 4\n",
      "clustering done in 0.10 ± 0.02 s \n",
      "Homogeneity: 0.657 ± 0.084\n",
      "Completeness: 0.727 ± 0.088\n",
      "V-measure: 0.690 ± 0.085\n",
      "Adjusted Rand-Index: 0.541 ± 0.109\n",
      "Silhouette Coefficient: 0.093 ± 0.002\n",
      "Number of elements asigned to each cluster: [199 701 701 196 187], sse= 309218\n",
      "(1984, 5)\n",
      "(1984,)\n",
      "(1984,)\n"
     ]
    }
   ],
   "source": [
    "nmfile = 'data-after-preprocess-experiment2'\n",
    "df = pd.read_csv(nmfile+'.csv')\n",
    "nmkolom = df.columns\n",
    "\n",
    "print (nmkolom)\n",
    "\n",
    "kolomproduk = 'namaproduk'\n",
    "kolomdeskripsi = 'desc'\n",
    "\n",
    "nmproduk = df[kolomproduk].tolist()\n",
    "deskripsi = df[kolomdeskripsi].tolist()\n",
    "\n",
    "kategori = df['kategori'].tolist()\n",
    "subkategori = df['subkategori'].tolist()\n",
    "\n",
    "rawdataset = []\n",
    "\n",
    "def get_dataset():\n",
    "    global df\n",
    "    df = df.reset_index()  # make sure indexes pair with number of rows\n",
    "    testdata = ''\n",
    "    i = 0\n",
    "    iambil = 1650\n",
    "    sentences = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        sebaris1 = row[kolomproduk]\n",
    "        if (pd.isna(sebaris1)):\n",
    "            sebaris1 = ' '\n",
    "\n",
    "        sebaris2 = row[kolomdeskripsi]\n",
    "        if (pd.isna(sebaris2)):\n",
    "            sebaris2 = ' '\n",
    "\n",
    "        kalimat = sebaris1 + ' ' + sebaris2\n",
    "        if (i==iambil):\n",
    "            testdata = kalimat\n",
    "            \n",
    "        sentences.append(kalimat)\n",
    "        i = i + 1\n",
    "\n",
    "    return (testdata, sentences)\n",
    "\n",
    "testdata, dataset_data = get_dataset()\n",
    "\n",
    "\n",
    "labels = kategori\n",
    "unique_labels, category_sizes = np.unique(labels, return_counts=True)\n",
    "true_k = unique_labels.shape[0]\n",
    "\n",
    "print(f\"{len(dataset_data)} documents - {true_k} categories\")\n",
    "print (unique_labels)\n",
    "\n",
    "\n",
    "evaluations = []\n",
    "evaluations_std = []\n",
    "\n",
    "\n",
    "def fit_and_evaluate(km, X, kateg_sizes, name=None, n_runs=5):\n",
    "    name = km._class.name_ if name is None else name\n",
    "\n",
    "    train_times = []\n",
    "    scores = defaultdict(list)\n",
    "    seed_sse_terkecil = 0\n",
    "    sse_lama = 999999\n",
    "    for seed in range(n_runs):\n",
    "        km.set_params(random_state=seed)\n",
    "        t0 = time()\n",
    "        km.fit(X)\n",
    "        train_times.append(time() - t0)\n",
    "        cluster_ids, cluster_sizes = np.unique(km.labels_, return_counts=True)\n",
    "        sse = 0\n",
    "        for i in range(len(kateg_sizes)):\n",
    "            sse = sse + pow(cluster_sizes[i] - kateg_sizes[i], 2)\n",
    "            \n",
    "        if (sse<sse_lama):\n",
    "            seed_sse_terkecil = seed \n",
    "        sse_lama = sse \n",
    "        \n",
    "        print(f\"Number of elements asigned to each cluster: {cluster_sizes}, sse= {sse}, seed= {seed}\") \n",
    "\n",
    "        scores[\"Homogeneity\"].append(metrics.homogeneity_score(labels, km.labels_))\n",
    "        scores[\"Completeness\"].append(metrics.completeness_score(labels, km.labels_))\n",
    "        scores[\"V-measure\"].append(metrics.v_measure_score(labels, km.labels_))\n",
    "        scores[\"Adjusted Rand-Index\"].append(\n",
    "            metrics.adjusted_rand_score(labels, km.labels_)\n",
    "        )\n",
    "        scores[\"Silhouette Coefficient\"].append(\n",
    "            metrics.silhouette_score(X, km.labels_, sample_size=2000)\n",
    "        )\n",
    "    train_times = np.asarray(train_times)\n",
    "\n",
    "    print(f\"clustering done in {train_times.mean():.2f} ± {train_times.std():.2f} s \")\n",
    "    evaluation = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.mean(),\n",
    "    }\n",
    "    evaluation_std = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.std(),\n",
    "    }\n",
    "    for score_name, score_values in scores.items():\n",
    "        mean_score, std_score = np.mean(score_values), np.std(score_values)\n",
    "        print(f\"{score_name}: {mean_score:.3f} ± {std_score:.3f}\")\n",
    "        evaluation[score_name] = mean_score\n",
    "        evaluation_std[score_name] = std_score\n",
    "    evaluations.append(evaluation)\n",
    "    evaluations_std.append(evaluation_std)\n",
    "    return seed_sse_terkecil\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=5, stop_words=\"english\")\n",
    "t0 = time()\n",
    "X_tfidf = vectorizer.fit_transform(dataset_data)\n",
    "\n",
    "print(f\"vectorization done in {time() - t0:.3f} s\")\n",
    "print(f\"n_samples: {X_tfidf.shape[0]}, n_features: {X_tfidf.shape[1]}\")\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '8'\n",
    "\n",
    "print (\"\\n*** KMeans with LSA on tf-idf vectors:\")\n",
    "print(\n",
    "    \"True number of documents in each category according to the class labels: \"\n",
    "    f\"{category_sizes}\"\n",
    ")\n",
    "\n",
    "print (\"Performing dimensionality reduction using LSA ...\")\n",
    "lsa = make_pipeline(TruncatedSVD(n_components=100), Normalizer(copy=False))\n",
    "t0 = time()\n",
    "X_lsa = lsa.fit_transform(X_tfidf)\n",
    "explained_variance = lsa[0].explained_variance_ratio_.sum()\n",
    "\n",
    "print(f\"LSA done in {time() - t0:.3f} s\")\n",
    "print(f\"Explained variance of the SVD step: {explained_variance * 100:.1f}%\")\n",
    "\n",
    "kmeans = KMeans(n_clusters=true_k, max_iter=100, n_init=5)\n",
    "\n",
    "seed_pilih = fit_and_evaluate(kmeans, X_lsa, category_sizes, name=\"KMeans\\non tf-idf vectors\")\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=true_k, max_iter=100, n_init=5, random_state=seed_pilih)\n",
    "kmeans.fit(X_lsa)\n",
    "centroids = kmeans.cluster_centers_\n",
    "cluster_ids, cluster_sizes = np.unique(kmeans.labels_, return_counts=True)\n",
    "labels = kmeans.labels_\n",
    "sse = 0\n",
    "for i in range(len(category_sizes)):\n",
    "    sse = sse + pow(cluster_sizes[i] - category_sizes[i], 2)\n",
    "\n",
    "print(f\"Number of elements asigned to each cluster: {cluster_sizes}, sse= {sse}\")\n",
    "\n",
    "def distance_from_center(X, centroids):\n",
    "\n",
    "\tdistance=euclidean_distances(X, np.array(centroids) )\n",
    "\n",
    "\thasil = np.min(distance, axis=1)\n",
    "\tprint (distance.shape)\n",
    "\tprint (hasil.shape)\n",
    "\n",
    "\treturn hasil   \n",
    "    \n",
    "euc_res=distance_from_center(X_lsa, centroids)\n",
    "\n",
    "# normalize the result  \n",
    "normlaized_res= euc_res  \n",
    "dfx = pd.DataFrame(normlaized_res, columns=columnName)\n",
    "dfx['cluster'] = labels\n",
    "dfx['kategori'] = kategori\n",
    "\n",
    "print (normlaized_res.shape)\n",
    "\n",
    "jumlahNpertama = 50\n",
    "\n",
    "for clusterx in cluster_ids:\n",
    "    dfy = dfx[dfx['cluster'] == clusterx]\n",
    "    dfz = dfy.sort_values(by=[nmkol], ascending=False)\n",
    "    # misal ditampilkan top ten outliers\n",
    "    outliers_idx = dfz.head(jumlahNpertama).index\n",
    "    #outliers = dfx[dfx.index.isin(outliers_idx)]\n",
    "    for idx in outliers_idx:\n",
    "        distance  = dfx.iloc[idx][nmkol]  \n",
    "        kategorix = df.iloc[idx]['kategori']   \n",
    "        nmproduk = df.iloc[idx]['namaproduk']\n",
    "        nmproduk = nmproduk[0:30]\n",
    "        \n",
    "dfx.to_csv(nmfile+'-cluster-LSA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7800d9b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5be9171",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Extraction and Clustering using experiment III"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3e48f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['namaproduk', 'desc', 'harga', 'terjual', 'rating', 'namatoko',\n",
      "       'kategori', 'subkategori', 'marketplace'],\n",
      "      dtype='object')\n",
      "2000 documents - 5 categories\n",
      "['ATK' 'Elektronik' 'Kesehatan' 'Kosmetik' 'Pakaian']\n",
      "vectorization done in 0.418 s\n",
      "n_samples: 2000, n_features: 4259\n",
      "\n",
      "*** KMeans with LSA on tf-idf vectors:\n",
      "True number of documents in each category according to the class labels: [400 400 400 400 400]\n",
      "Performing dimensionality reduction using LSA ...\n",
      "LSA done in 0.489 s\n",
      "Explained variance of the SVD step: 46.5%\n",
      "Number of elements asigned to each cluster: [266 351 293 673 417], sse= 106624, seed= 0\n",
      "Number of elements asigned to each cluster: [  99  480  193 1128  100], sse= 759834, seed= 1\n",
      "Number of elements asigned to each cluster: [760 435 513 192 100], sse= 276858, seed= 2\n",
      "Number of elements asigned to each cluster: [625 485 294 388 208], sse= 106094, seed= 3\n",
      "Number of elements asigned to each cluster: [193 473 381 853 100], sse= 343748, seed= 4\n",
      "clustering done in 0.12 ± 0.01 s \n",
      "Homogeneity: 0.590 ± 0.085\n",
      "Completeness: 0.667 ± 0.050\n",
      "V-measure: 0.625 ± 0.069\n",
      "Adjusted Rand-Index: 0.486 ± 0.110\n",
      "Silhouette Coefficient: 0.086 ± 0.002\n",
      "Number of elements asigned to each cluster: [625 485 294 388 208], sse= 106094\n",
      "(2000, 5)\n",
      "(2000,)\n",
      "(2000,)\n"
     ]
    }
   ],
   "source": [
    "nmfile = 'datas-after-preprocess-experiment3'\n",
    "df = pd.read_csv(nmfile+'.csv')\n",
    "nmkolom = df.columns\n",
    "\n",
    "print (nmkolom)\n",
    "\n",
    "kolomproduk = 'namaproduk'\n",
    "kolomdeskripsi = 'desc'\n",
    "\n",
    "nmproduk = df[kolomproduk].tolist()\n",
    "deskripsi = df[kolomdeskripsi].tolist()\n",
    "\n",
    "kategori = df['kategori'].tolist()\n",
    "subkategori = df['subkategori'].tolist()\n",
    "\n",
    "rawdataset = []\n",
    "\n",
    "def get_dataset():\n",
    "    global df\n",
    "    df = df.reset_index()  # make sure indexes pair with number of rows\n",
    "    testdata = ''\n",
    "    i = 0\n",
    "    iambil = 1650\n",
    "    sentences = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        sebaris1 = row[kolomproduk]\n",
    "        if (pd.isna(sebaris1)):\n",
    "            sebaris1 = ' '\n",
    "\n",
    "        sebaris2 = row[kolomdeskripsi]\n",
    "        if (pd.isna(sebaris2)):\n",
    "            sebaris2 = ' '\n",
    "\n",
    "        kalimat = sebaris1 + ' ' + sebaris2\n",
    "        if (i==iambil):\n",
    "            testdata = kalimat\n",
    "            \n",
    "        sentences.append(kalimat)\n",
    "        i = i + 1\n",
    "\n",
    "    return (testdata, sentences)\n",
    "\n",
    "testdata, dataset_data = get_dataset()\n",
    "\n",
    "\n",
    "labels = kategori\n",
    "unique_labels, category_sizes = np.unique(labels, return_counts=True)\n",
    "true_k = unique_labels.shape[0]\n",
    "\n",
    "print(f\"{len(dataset_data)} documents - {true_k} categories\")\n",
    "print (unique_labels)\n",
    "\n",
    "\n",
    "evaluations = []\n",
    "evaluations_std = []\n",
    "\n",
    "\n",
    "def fit_and_evaluate(km, X, kateg_sizes, name=None, n_runs=5):\n",
    "    name = km._class.name_ if name is None else name\n",
    "\n",
    "    train_times = []\n",
    "    scores = defaultdict(list)\n",
    "    seed_sse_terkecil = 0\n",
    "    sse_lama = 999999\n",
    "    for seed in range(n_runs):\n",
    "        km.set_params(random_state=seed)\n",
    "        t0 = time()\n",
    "        km.fit(X)\n",
    "        train_times.append(time() - t0)\n",
    "        cluster_ids, cluster_sizes = np.unique(km.labels_, return_counts=True)\n",
    "        sse = 0\n",
    "        for i in range(len(kateg_sizes)):\n",
    "            sse = sse + pow(cluster_sizes[i] - kateg_sizes[i], 2)\n",
    "            \n",
    "        if (sse<sse_lama):\n",
    "            seed_sse_terkecil = seed \n",
    "        sse_lama = sse \n",
    "        \n",
    "        print(f\"Number of elements asigned to each cluster: {cluster_sizes}, sse= {sse}, seed= {seed}\") \n",
    "\n",
    "        scores[\"Homogeneity\"].append(metrics.homogeneity_score(labels, km.labels_))\n",
    "        scores[\"Completeness\"].append(metrics.completeness_score(labels, km.labels_))\n",
    "        scores[\"V-measure\"].append(metrics.v_measure_score(labels, km.labels_))\n",
    "        scores[\"Adjusted Rand-Index\"].append(\n",
    "            metrics.adjusted_rand_score(labels, km.labels_)\n",
    "        )\n",
    "        scores[\"Silhouette Coefficient\"].append(\n",
    "            metrics.silhouette_score(X, km.labels_, sample_size=2000)\n",
    "        )\n",
    "    train_times = np.asarray(train_times)\n",
    "\n",
    "    print(f\"clustering done in {train_times.mean():.2f} ± {train_times.std():.2f} s \")\n",
    "    evaluation = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.mean(),\n",
    "    }\n",
    "    evaluation_std = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.std(),\n",
    "    }\n",
    "    for score_name, score_values in scores.items():\n",
    "        mean_score, std_score = np.mean(score_values), np.std(score_values)\n",
    "        print(f\"{score_name}: {mean_score:.3f} ± {std_score:.3f}\")\n",
    "        evaluation[score_name] = mean_score\n",
    "        evaluation_std[score_name] = std_score\n",
    "    evaluations.append(evaluation)\n",
    "    evaluations_std.append(evaluation_std)\n",
    "    return seed_sse_terkecil\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=5, stop_words=\"english\")\n",
    "t0 = time()\n",
    "X_tfidf = vectorizer.fit_transform(dataset_data)\n",
    "\n",
    "print(f\"vectorization done in {time() - t0:.3f} s\")\n",
    "print(f\"n_samples: {X_tfidf.shape[0]}, n_features: {X_tfidf.shape[1]}\")\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '8'\n",
    "\n",
    "print (\"\\n*** KMeans with LSA on tf-idf vectors:\")\n",
    "print(\n",
    "    \"True number of documents in each category according to the class labels: \"\n",
    "    f\"{category_sizes}\"\n",
    ")\n",
    "\n",
    "print (\"Performing dimensionality reduction using LSA ...\")\n",
    "lsa = make_pipeline(TruncatedSVD(n_components=100), Normalizer(copy=False))\n",
    "t0 = time()\n",
    "X_lsa = lsa.fit_transform(X_tfidf)\n",
    "explained_variance = lsa[0].explained_variance_ratio_.sum()\n",
    "\n",
    "print(f\"LSA done in {time() - t0:.3f} s\")\n",
    "print(f\"Explained variance of the SVD step: {explained_variance * 100:.1f}%\")\n",
    "\n",
    "kmeans = KMeans(n_clusters=true_k, max_iter=100, n_init=5)\n",
    "\n",
    "seed_pilih = fit_and_evaluate(kmeans, X_lsa, category_sizes, name=\"KMeans\\non tf-idf vectors\")\n",
    "\n",
    "\n",
    "kmeans = KMeans(n_clusters=true_k, max_iter=100, n_init=5, random_state=seed_pilih)\n",
    "kmeans.fit(X_lsa)\n",
    "centroids = kmeans.cluster_centers_\n",
    "cluster_ids, cluster_sizes = np.unique(kmeans.labels_, return_counts=True)\n",
    "labels = kmeans.labels_\n",
    "sse = 0\n",
    "for i in range(len(category_sizes)):\n",
    "    sse = sse + pow(cluster_sizes[i] - category_sizes[i], 2)\n",
    "\n",
    "print(f\"Number of elements asigned to each cluster: {cluster_sizes}, sse= {sse}\")\n",
    "\n",
    "def distance_from_center(X, centroids):\n",
    "\n",
    "\tdistance=euclidean_distances(X, np.array(centroids) )\n",
    "\n",
    "\thasil = np.min(distance, axis=1)\n",
    "\tprint (distance.shape)\n",
    "\tprint (hasil.shape)\n",
    "\n",
    "\treturn hasil   \n",
    "    \n",
    "euc_res=distance_from_center(X_lsa, centroids)\n",
    "\n",
    "# normalize the result  \n",
    "normlaized_res= euc_res \n",
    "dfx = pd.DataFrame(normlaized_res, columns=columnName)\n",
    "dfx['cluster'] = labels\n",
    "dfx['kategori'] = kategori\n",
    "\n",
    "print (normlaized_res.shape)\n",
    "\n",
    "jumlahNpertama = 50\n",
    "\n",
    "for clusterx in cluster_ids:\n",
    "    dfy = dfx[dfx['cluster'] == clusterx]\n",
    "    dfz = dfy.sort_values(by=[nmkol], ascending=False)\n",
    "    # misal ditampilkan top ten outliers\n",
    "    outliers_idx = dfz.head(jumlahNpertama).index\n",
    "    #outliers = dfx[dfx.index.isin(outliers_idx)]\n",
    "    for idx in outliers_idx:\n",
    "        distance  = dfx.iloc[idx][nmkol]  \n",
    "        kategorix = df.iloc[idx]['kategori']   \n",
    "        nmproduk = df.iloc[idx]['namaproduk']\n",
    "        nmproduk = nmproduk[0:30]\n",
    "        \n",
    "dfx.to_csv(nmfile+'-cluster-LSA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79237c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
